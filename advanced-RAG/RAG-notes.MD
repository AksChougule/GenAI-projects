Retrieval Augmented Generation are here to stay, considering the practicality of thr RAG systems. Building a basic RAG is a fairly straightforward process and can be done in an hour or two. However building a reliable RAG system can add immense value, but not very straight forward. This is an attempt to collect a comprehensive working copy of the factors that can affect the performance of RAGs, allowing us to build improved versions of baseline RAGs and also have a better debugging playbook.

### Stage 1: Better Data Pre-processing and storage
- Clean data: Even though it seems obvious, if the data is inaccurate, outdated, incomplete, or biased, the RAG system can not generate reliable responses. So the data must be as clean as possible. This could include removing duplicates or outdated entries, fixing potential inaccuracies, etc.
- Data format: Data format can be of great help. If you have tried extracting tables from pdfs, then dealing with corner cases like tables with nested headers, page breaks, merged cells, sparse information can be tedious to extract. However if you convert the pdf to html, then try the extraction then you get better results. [video demo with Unstructured]
- Chunk size: If a chunk is too short, it might not contain sufficient context. But if it is too large, it might contain too much irrelevant information. A better way to strike the balance is based on your use case - if it is question answering, you may need shorter specific chunks, but if your use case is summarization, you may need longer chunks. Also, overlap size between adjacent chunks helps with possible information split scenarios.
- Recursive Chunking: Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators. If the initial attempt at splitting the text doesn’t produce chunks of the desired size or structure, the method recursively calls itself on the resulting chunks with a different separator or criterion until the desired chunk size or structure is achieved. This means that while the chunks aren’t going to be exactly the same size, they’ll still “aspire” to be of a similar size.
- Context-aware Chunking: What if we could better than having overlapped chunks? It turns out there is some good work happening in creating context-aware chunks [ video ]
- Domain-specific chunking: A new medical document splitter by @John Snow labs enables you to tune the process for clinical & biomedical documents, including using a pre-trained model for "ill-formatted" documents (i.e., the majority of clinical notes). [ video ]
- Semantic chunking: Doing distance between embeddings of sequential chunks. If the distance is large enough, make a chunk cut off. Hypothesis: Using embeddings of individual sentences, you can find semantic "break points" by measuring distances of sequential sentences. If the distance between sentences is high, then count that as a breakpoint. Taking distances of individual sentences was too noisy, so I grouped them together in sets of 3, this made it easier to work with. [ tweet ]
- Agentic Chunking: Agentic Chunking is about taking the semantic chunking further. This involves attempt to implemnet how we would we chunk a document by hand. 1. Get propositions 2. For each proposition, ask the LLM, should this be in an existing chunk? or a make a new one? [ tweet ]
- Proposition level chunkin and indexing: segmenting and indexing a retrieval corpus on the proposition level can be a simple yet effective strategy to increase dense retrievers’ generalization performance at inference time [ demo | paper | code]
- Metadata: A very effective strategy for improving retrieval is to add meta-data to your chunks, and then use it to help process results. Date is a common meta-data tag to add because it allows you to filter by recency. Imagine you are building an app that allows users to query their email history. It’s likely that more recent emails will be more relevant. 
- Embedding models: Embedding models are at the core of your retrieval. The quality of your embeddings heavily impacts your retrieval results. Usually, the higher the dimensionality of the generated embeddings, the higher the precision of your embeddings. For an idea of what alternative embedding models are available, you can look at the Massive Text Embedding Benchmark (MTEB) Leaderboard, which covers 164 text embedding models (at the time of this writing).
- Multi-indexing: If the metadata is not sufficient enough to provide additional information to separate different types of context logically, you may want to experiment with multiple indexes [1, 9]. For example, you can use different indexes for different types of documents. Note that you will have to incorporate some index routing at retrieval time [1, 9]. If you are interested in a deeper dive into metadata and separate collections, you might want to learn more about the concept of native multi-tenancy.
- Indexing algorithms: To enable lightning-fast similarity search at scale, vector databases and vector indexing libraries use an Approximate Nearest Neighbor (ANN) search instead of a k-nearest neighbor (kNN) search. As the name suggests, ANN algorithms approximate the nearest neighbors and thus can be less precise than a kNN algorithm. There are different ANN algorithms you could experiment with, such as Facebook Faiss (clustering), Spotify Annoy (trees), Google ScaNN (vector compression), and HNSWLIB (proximity graphs). Also, many of these ANN algorithms have some parameters you could tune, such as ef, efConstruction, and maxConnections for HNSW [1]. (more here)

### Stage 2: Better User Queries:
- Rephrasing (manual):  if your system doesn’t find relevant context for the query, you can have the LLM rephrase the query and try again. Two questions that seem the same to humans don’t always look that similar in embedding space.
- Query expansion: Adding more details so it can generate a better vector.
- Re-writing (automated): Query re-writing uses LLM to reformulate initial query in order to improve retrieval. Both LangChain and LlamaIndex have 
- Query routing: It’s often useful to have more than one index. You then route queries to the appropriate index when they come in. For example, you may have one index that handles summarization questions, another that handles pointed questions, and another that works well for date sensitive questions. If you try to optimize one index for all of these behaviors, you’ll end up compromising how well it does at all of them. Instead you can route the query to the proper index. 
- HyDE: HyDE is a strategy which takes a query, generates a hypothetical response, and then uses both for embedding look up. Researchers have found this can dramatically improve performance.
- Sub-queries: LLMs tend to work better when they break down complex queries. You can build this into your RAG system such that a query is decomposed into multiple questions. [same as MultiQuery from LangChain, Sub-queries is a LlamaIndex term]

### Stage 3: Better Search:
- Different similarity methods: The basic rule of thumb in selecting the best similarity metric for your index (e.g. Pinecone index) is to match it to the one used to train your embedding model. For example, the all-MiniLM-L6-v2 model was trained using cosine similarity — so using cosine similarity for the index will produce the most accurate result. If you used a Euclidean distance measure to train your model, the same similarity metric should be used in the index, etc. 
- Hybrid Search (Vector + Keywords): To ensure the RAG outputs are as relevant as possible, semantic search technique can be combined with keyword search to further refine results. This hybrid approach helps retrieve data based on its meaning or intent and ensure the right contextual data can be retrieved. For example, Salesforce’s Einstein Copilot Search is implemented with this approach.
A relatively old idea that you could take the best from both worlds — keyword-based old school search — sparse retrieval algorithms like tf-idf or search industry standard BM25 — and modern semantic or vector search and combine it in one retrieval result.
The only trick here is to properly combine the retrieved results with different similarity scores — this problem is usually solved with the help of the Reciprocal Rank Fusion algorithm, reranking the retrieved results for the final output.
- Hybrid Search (Vector + Graph): A knowledge graph is a deterministic mapping of relationships between concepts and entities. Unlike a similarity search in a vector database, a knowledge graph can consistently and accurately retrieve related rules and concepts, and dramatically reduce hallucinations. (paper + video)
- Scaling the search: Although the LLM can extract relevant chunks of text from a vector database, you can improve the speed and reliability of retrieval by using a document hierarchy as a pre-processing step to locate the most relevant chunks of text. This strategy improves retrieval reliability, speed, repeatability, and can help reduce hallucinations due to chunk extraction issues. [ blog ] { insert image }

### Stage 4: Retrieval
- Re-ranking: Reranking is one solution to the issue of discrepancy between similarity and relevance. With reranking, your retrieval system gets the top nodes for context as usual. It then re-ranks them based on relevance. Cohere Rereanker is commonly used for this. This strategy is one I see experts recommend often. No matter the use case, if you’re building with RAG, you should experiment with reranking and see if it improves your system. Both LangChain and LlamaIndex have abstractions that make it easy to set up.
- Multi-stage Document Re-ranking: It is often a good strategy to plug in 𝑫𝒐𝒄𝒖𝒎𝒆𝒏𝒕 𝑹𝒆-𝒓𝒂𝒏𝒌𝒊𝒏𝒈 𝒎𝒆𝒕𝒉𝒐𝒅𝒔 on top of M0 (Vector DB score or TF-IDF or BM25) for getting the most relevant query answerable documents to the top. Hence, passing good quality documents to LLMs. Multi-stage Document Re-ranking strategies like MonoBERT/MonoT5 (Pointwise ranking) and duoBERT/duoT5 (Pairwise ranking) can be a good fit here. [video | paper]
- Parent window retrieval: This strategy is part of small-to-big retrieval and involves dividing each chunk (parent chunk) into sever child chunks recursively (1024 into 2*512, 4*256 and 8*128). [video | code]
- Sentence window retrieval: This strategy is part of small-to-big retrieval and involves dividing each chunk (parent chunk) into sever child chunks recursively (1024 into 2*512, 4*256 and 8*128). [course | video | code]
- Auto-merging retrieval: Auto-merging retrieval focuses on amalgamating information from various text segments to formulate a response that is both extensive and contextually relevant to a query. This method is especially valuable when no single document or segment completely addresses the query. Instead, the solution is found by integrating information from multiple sources. It facilitates the merging of smaller pieces into larger segments. [course | code]
- Self-querying retriever: When a user inputs their query, a large language model is employed to reshape it. This not only captures the semantic aspects of the query but also transforms it to facilitate searches on metadata. This is crucial, for example, in scenarios like searching for a movie where specifying the year is important. In such cases, one wouldn't rely on semantic search via a vector store to find the year, but rather on a more direct method. [ video | code ]
- MultiQueryRetriever: In these vector database retrievals, as queries are represented in a high-dimensional space, and similar documents are identified based on their "distance" from the query. Naturally, this method can yield varying results due to minor variations in the wording of queries or if the embeddings fail to accurately capture the data's semantics. To counter these issues, prompt engineering or tuning is often employed, although it can be a cumbersome process. The MultiQueryRetriever streamlines this by using a Large Language Model (LLM) to generate a variety of queries from different angles based on a single user input. For each generated query, it retrieves a relevant set of documents and then combines these sets, ensuring a broader range of potentially pertinent documents is covered. [ code ]
- The FilCo method (filtering context) is introduced as a solution to improve the quality of context for generators. It filters out irrelevant content (marked in red) and leaves precisely supporting content, making it easier for the generator to predict the correct answer. [ paper ]

