Retrieval Augmented Generation are here to stay, considering the practicality of thr RAG systems. Building a basic RAG is a fairly straightforward process and can be done in an hour or two. However building a reliable RAG system can add immense value, but not very straight forward. This is an attempt to collect a comprehensive working copy of the factors that can affect the performance of RAGs, allowing us to build improved versions of baseline RAGs and also have a better debugging playbook.

### Stage 1: Better Data Pre-processing and storage
- Clean data: Even though it seems obvious, if the data is inaccurate, outdated, incomplete, or biased, the RAG system can not generate reliable responses. So the data must be as clean as possible. This could include removing duplicates or outdated entries, fixing potential inaccuracies, etc.
- Data format: Data format can be of great help. If you have tried extracting tables from pdfs, then dealing with corner cases like tables with nested headers, page breaks, merged cells, sparse information can be tedious to extract. However if you convert the pdf to html, then try the extraction then you get better results. [video demo with Unstructured]
- Chunk size: If a chunk is too short, it might not contain sufficient context. But if it is too large, it might contain too much irrelevant information. A better way to strike the balance is based on your use case - if it is question answering, you may need shorter specific chunks, but if your use case is summarization, you may need longer chunks. Also, overlap size between adjacent chunks helps with possible information split scenarios.
- Recursive Chunking: Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators. If the initial attempt at splitting the text doesn’t produce chunks of the desired size or structure, the method recursively calls itself on the resulting chunks with a different separator or criterion until the desired chunk size or structure is achieved. This means that while the chunks aren’t going to be exactly the same size, they’ll still “aspire” to be of a similar size.
- Context-aware Chunking: What if we could better than having overlapped chunks? It turns out there is some good work happening in creating context-aware chunks [ video ]
- Domain-specific chunking: A new medical document splitter by @John Snow labs enables you to tune the process for clinical & biomedical documents, including using a pre-trained model for "ill-formatted" documents (i.e., the majority of clinical notes). [ video ]
- Semantic chunking: Doing distance between embeddings of sequential chunks. If the distance is large enough, make a chunk cut off. Hypothesis: Using embeddings of individual sentences, you can find semantic "break points" by measuring distances of sequential sentences. If the distance between sentences is high, then count that as a breakpoint. Taking distances of individual sentences was too noisy, so I grouped them together in sets of 3, this made it easier to work with. [ tweet ]
- Agentic Chunking: Agentic Chunking is about taking the semantic chunking further. This involves attempt to implemnet how we would we chunk a document by hand. 1. Get propositions 2. For each proposition, ask the LLM, should this be in an existing chunk? or a make a new one? [ tweet ]
- Proposition level chunkin and indexing: segmenting and indexing a retrieval corpus on the proposition level can be a simple yet effective strategy to increase dense retrievers’ generalization performance at inference time [ demo | paper | code]
- Metadata: A very effective strategy for improving retrieval is to add meta-data to your chunks, and then use it to help process results. Date is a common meta-data tag to add because it allows you to filter by recency. Imagine you are building an app that allows users to query their email history. It’s likely that more recent emails will be more relevant. 
- Embedding models: Embedding models are at the core of your retrieval. The quality of your embeddings heavily impacts your retrieval results [1, 4]. Usually, the higher the dimensionality of the generated embeddings, the higher the precision of your embeddings. For an idea of what alternative embedding models are available, you can look at the Massive Text Embedding Benchmark (MTEB) Leaderboard, which covers 164 text embedding models (at the time of this writing).
- Multi-indexing: If the metadata is not sufficient enough to provide additional information to separate different types of context logically, you may want to experiment with multiple indexes [1, 9]. For example, you can use different indexes for different types of documents. Note that you will have to incorporate some index routing at retrieval time [1, 9]. If you are interested in a deeper dive into metadata and separate collections, you might want to learn more about the concept of native multi-tenancy.
- Indexing algorithms: To enable lightning-fast similarity search at scale, vector databases and vector indexing libraries use an Approximate Nearest Neighbor (ANN) search instead of a k-nearest neighbor (kNN) search. As the name suggests, ANN algorithms approximate the nearest neighbors and thus can be less precise than a kNN algorithm. There are different ANN algorithms you could experiment with, such as Facebook Faiss (clustering), Spotify Annoy (trees), Google ScaNN (vector compression), and HNSWLIB (proximity graphs). Also, many of these ANN algorithms have some parameters you could tune, such as ef, efConstruction, and maxConnections for HNSW [1]. (more here)
