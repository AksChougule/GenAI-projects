{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fceb8f7",
   "metadata": {},
   "source": [
    "Author: Akshay Chougule\n",
    "\n",
    "Originally Created On: Nov 19, 2023\n",
    "\n",
    "Credit: Using the [mlflow documentation](https://mlflow.org/docs/latest/model-evaluation/index.html) experiment tracking\n",
    "\n",
    "Goal of the notebook:\n",
    "- LangChain setup for a RAG-based system using a web page/ web doc \n",
    "- Learn expriment tracking with LLMs\n",
    "- Learn to use SoTA LLMs to evaluate the responses of LLM under evaluation (Yes, LLM to evaluate a LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad79ef8-1a39-4051-b6f9-ec80d3b191f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96336e9-1ded-4010-a030-51817d04b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py39/lib/python3.9/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/py39/lib/python3.9/site-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc7a932-f8ce-479b-ab0c-290e6fda2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/ubuntu/codebase/my_github/generative-ai-experiments/')\n",
    "from Constants import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f891f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd6a90e-b2ff-4b72-94c3-a7518783c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02f93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://mlflow.org/docs/latest/index.html\")\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad916ff",
   "metadata": {},
   "source": [
    "### Evaluate the RAG system using mlflow.evaluate()\n",
    "\n",
    "Create a simple function that runs each input through the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c677cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_df):\n",
    "    answer = []\n",
    "    for index, row in input_df.iterrows():\n",
    "        answer.append(qa(row[\"questions\"]))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d62492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an eval dataset\n",
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"questions\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"How to run mlflow.evaluate()?\",\n",
    "            \"How to log_table()?\",\n",
    "            \"How to load_table()?\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d6af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package mlflow.metrics.genai in mlflow.metrics:\n",
      "\n",
      "NAME\n",
      "    mlflow.metrics.genai\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    genai_metric\n",
      "    metric_definitions\n",
      "    model_utils\n",
      "    prompt_template\n",
      "    prompts (package)\n",
      "    utils\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        mlflow.metrics.genai.base.EvaluationExample\n",
      "    \n",
      "    class EvaluationExample(builtins.object)\n",
      "     |  EvaluationExample(input: str, output: str, score: float, justification: str, grading_context: Union[Dict[str, str], str, NoneType] = None) -> None\n",
      "     |  \n",
      "     |  .. Note:: Experimental: This class may change or be removed in a future release without warning.\n",
      "     |  \n",
      "     |  \n",
      "     |  Stores the sample example during few shot learning during LLM evaluation\n",
      "     |  \n",
      "     |  :param input: The input provided to the model\n",
      "     |  :param output: The output generated by the model\n",
      "     |  :param score: The score given by the evaluator\n",
      "     |  :param justification: The justification given by the evaluator\n",
      "     |  :param grading_context: The grading_context provided to the evaluator for evaluation. Either\n",
      "     |                          a dictionary of grading context column names and grading context strings\n",
      "     |                          or a single grading context string.\n",
      "     |  \n",
      "     |  .. code-block:: python\n",
      "     |      :caption: Example for creating an EvaluationExample\n",
      "     |  \n",
      "     |      from mlflow.metrics.base import EvaluationExample\n",
      "     |  \n",
      "     |      example = EvaluationExample(\n",
      "     |          input=\"What is MLflow?\",\n",
      "     |          output=\"MLflow is an open-source platform for managing machine \"\n",
      "     |          \"learning workflows, including experiment tracking, model packaging, \"\n",
      "     |          \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
      "     |          score=4,\n",
      "     |          justification=\"The definition effectively explains what MLflow is \"\n",
      "     |          \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
      "     |          grading_context={\n",
      "     |              \"ground_truth\": \"MLflow is an open-source platform for managing \"\n",
      "     |              \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
      "     |              \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
      "     |              \"designed to address the challenges that data scientists and machine learning \"\n",
      "     |              \"engineers face when developing, training, and deploying machine learning models.\"\n",
      "     |          },\n",
      "     |      )\n",
      "     |      print(str(example))\n",
      "     |  \n",
      "     |  .. code-block:: text\n",
      "     |      :caption: Output\n",
      "     |  \n",
      "     |      Input: What is MLflow?\n",
      "     |      Provided output: \"MLflow is an open-source platform for managing machine \"\n",
      "     |          \"learning workflows, including experiment tracking, model packaging, \"\n",
      "     |          \"versioning, and deployment, simplifying the ML lifecycle.\"\n",
      "     |      Provided ground_truth: \"MLflow is an open-source platform for managing \"\n",
      "     |          \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
      "     |          \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
      "     |          \"designed to address the challenges that data scientists and machine learning \"\n",
      "     |          \"engineers face when developing, training, and deploying machine learning models.\"\n",
      "     |      Score: 4\n",
      "     |      Justification: \"The definition effectively explains what MLflow is \"\n",
      "     |          \"its purpose, and its developer. It could be more concise for a 5-score.\"\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __init__(self, input: str, output: str, score: float, justification: str, grading_context: Union[Dict[str, str], str, NoneType] = None) -> None\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'grading_context': typing.Union[typing.Dict[str, st...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'grading_context': Field(name='grading_context...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  grading_context = None\n",
      "\n",
      "FUNCTIONS\n",
      "    answer_correctness(model: Optional[str] = None, metric_version: Optional[str] = None, examples: Optional[List[mlflow.metrics.genai.base.EvaluationExample]] = None) -> mlflow.models.evaluation.base.EvaluationMetric\n",
      "        .. Note:: Experimental: This function may change or be removed in a future release without warning.\n",
      "        \n",
      "        \n",
      "        This function will create a genai metric used to evaluate the answer correctness of an LLM\n",
      "        using the model provided. Answer correctness will be assessed by the accuracy of the provided\n",
      "        output based on the ``ground_truth``, which should be specified in the ``targets`` column.\n",
      "        \n",
      "        The ``targets`` eval_arg must be provided as part of the input dataset or output\n",
      "        predictions. This can be mapped to a column of a different name using ``col_mapping``\n",
      "        in the ``evaluator_config`` parameter, or using the ``targets`` parameter in mlflow.evaluate().\n",
      "        \n",
      "        An MlflowException will be raised if the specified version for this metric does not exist.\n",
      "        \n",
      "        :param model: (Optional) Model uri of an openai or gateway judge model in the format of\n",
      "            \"openai:/gpt-4\" or \"gateway:/my-route\". Defaults to\n",
      "            \"openai:/gpt-4\". Your use of a third party LLM service (e.g., OpenAI) for\n",
      "            evaluation may be subject to and governed by the LLM service's terms of use.\n",
      "        :param metric_version: (Optional) The version of the answer correctness metric to use.\n",
      "            Defaults to the latest version.\n",
      "        :param examples: (Optional) Provide a list of examples to help the judge model evaluate the\n",
      "            answer correctness. It is highly recommended to add examples to be used as a reference to\n",
      "            evaluate the new results.\n",
      "        :return: A metric object\n",
      "    \n",
      "    answer_relevance(model: Optional[str] = None, metric_version: Optional[str] = 'v1', examples: Optional[List[mlflow.metrics.genai.base.EvaluationExample]] = None) -> mlflow.models.evaluation.base.EvaluationMetric\n",
      "        .. Note:: Experimental: This function may change or be removed in a future release without warning.\n",
      "        \n",
      "        \n",
      "        This function will create a genai metric used to evaluate the answer relevance of an LLM\n",
      "        using the model provided. Answer relevance will be assessed based on the appropriateness and\n",
      "        applicability of the output with respect to the input.\n",
      "        \n",
      "        An MlflowException will be raised if the specified version for this metric does not exist.\n",
      "        \n",
      "        :param model: (Optional) Model uri of an openai or gateway judge model in the format of\n",
      "            \"openai:/gpt-4\" or \"gateway:/my-route\". Defaults to\n",
      "            \"openai:/gpt-4\". Your use of a third party LLM service (e.g., OpenAI) for\n",
      "            evaluation may be subject to and governed by the LLM service's terms of use.\n",
      "        :param metric_version: (Optional) The version of the answer relevance metric to use.\n",
      "            Defaults to the latest version.\n",
      "        :param examples: (Optional) Provide a list of examples to help the judge model evaluate the\n",
      "            answer relevance. It is highly recommended to add examples to be used as a reference to\n",
      "            evaluate the new results.\n",
      "        :return: A metric object\n",
      "    \n",
      "    answer_similarity(model: Optional[str] = None, metric_version: Optional[str] = None, examples: Optional[List[mlflow.metrics.genai.base.EvaluationExample]] = None) -> mlflow.models.evaluation.base.EvaluationMetric\n",
      "        .. Note:: Experimental: This function may change or be removed in a future release without warning.\n",
      "        \n",
      "        \n",
      "        This function will create a genai metric used to evaluate the answer similarity of an LLM\n",
      "        using the model provided. Answer similarity will be assessed by the semantic similarity of the\n",
      "        output to the ``ground_truth``, which should be specified in the ``targets`` column.\n",
      "        \n",
      "        The ``targets`` eval_arg must be provided as part of the input dataset or output\n",
      "        predictions. This can be mapped to a column of a different name using ``col_mapping``\n",
      "        in the ``evaluator_config`` parameter, or using the ``targets`` parameter in mlflow.evaluate().\n",
      "        \n",
      "        An MlflowException will be raised if the specified version for this metric does not exist.\n",
      "        \n",
      "        :param model: (Optional) Model uri of an openai or gateway judge model in the format of\n",
      "            \"openai:/gpt-4\" or \"gateway:/my-route\". Defaults to\n",
      "            \"openai:/gpt-4\". Your use of a third party LLM service (e.g., OpenAI) for\n",
      "            evaluation may be subject to and governed by the LLM service's terms of use.\n",
      "        :param metric_version: (Optional) The version of the answer similarity metric to use.\n",
      "            Defaults to the latest version.\n",
      "        :param examples: (Optional) Provide a list of examples to help the judge model evaluate the\n",
      "            answer similarity. It is highly recommended to add examples to be used as a reference to\n",
      "            evaluate the new results.\n",
      "        :return: A metric object\n",
      "    \n",
      "    faithfulness(model: Optional[str] = None, metric_version: Optional[str] = 'v1', examples: Optional[List[mlflow.metrics.genai.base.EvaluationExample]] = None) -> mlflow.models.evaluation.base.EvaluationMetric\n",
      "        .. Note:: Experimental: This function may change or be removed in a future release without warning.\n",
      "        \n",
      "        \n",
      "        This function will create a genai metric used to evaluate the faithfullness of an LLM using the\n",
      "        model provided. Faithfulness will be assessed based on how factually consistent the output\n",
      "        is to the ``context``.\n",
      "        \n",
      "        The ``context`` eval_arg must be provided as part of the input dataset or output\n",
      "        predictions. This can be mapped to a column of a different name using ``col_mapping``\n",
      "        in the ``evaluator_config`` parameter.\n",
      "        \n",
      "        An MlflowException will be raised if the specified version for this metric does not exist.\n",
      "        \n",
      "        :param model: (Optional) Model uri of an openai or gateway judge model in the format of\n",
      "            \"openai:/gpt-4\" or \"gateway:/my-route\". Defaults to\n",
      "            \"openai:/gpt-4\". Your use of a third party LLM service (e.g., OpenAI) for\n",
      "            evaluation may be subject to and governed by the LLM service's terms of use.\n",
      "        :param metric_version: (Optional) The version of the faithfulness metric to use.\n",
      "            Defaults to the latest version.\n",
      "        :param examples: (Optional) Provide a list of examples to help the judge model evaluate the\n",
      "            faithfulness. It is highly recommended to add examples to be used as a reference to evaluate\n",
      "            the new results.\n",
      "        :return: A metric object\n",
      "    \n",
      "    make_genai_metric(name: str, definition: str, grading_prompt: str, examples: Optional[List[mlflow.metrics.genai.base.EvaluationExample]] = None, version: Optional[str] = 'v1', model: Optional[str] = 'openai:/gpt-4', grading_context_columns: Union[str, List[str], NoneType] = [], parameters: Optional[Dict[str, Any]] = None, aggregations: Optional[List[str]] = ['mean', 'variance', 'p90'], greater_is_better: bool = True, max_workers: int = 10) -> mlflow.models.evaluation.base.EvaluationMetric\n",
      "        .. Note:: Experimental: This function may change or be removed in a future release without warning.\n",
      "        \n",
      "        \n",
      "        Create a genai metric used to evaluate LLM using LLM as a judge in MLflow.\n",
      "        \n",
      "        :param name: Name of the metric.\n",
      "        :param definition: Definition of the metric.\n",
      "        :param grading_prompt: Grading criteria of the metric.\n",
      "        :param examples: (Optional) Examples of the metric.\n",
      "        :param version: (Optional) Version of the metric. Currently supported versions are: v1.\n",
      "        :param model: (Optional) Model uri of an openai or gateway judge model in the format of\n",
      "            \"openai:/gpt-4\" or \"gateway:/my-route\". Defaults to\n",
      "            \"openai:/gpt-4\". Your use of a third party LLM service (e.g., OpenAI) for\n",
      "            evaluation may be subject to and governed by the LLM service's terms of use.\n",
      "        :param grading_context_columns: (Optional) The name of the grading context column, or a list of\n",
      "            grading context column names, required to compute the metric. The\n",
      "            ``grading_context_columns`` are used by the LLM as a judge as additional information to\n",
      "            compute the metric. The columns are extracted from the input dataset or output predictions\n",
      "            based on ``col_mapping`` in the ``evaluator_config`` passed to :py:func:`mlflow.evaluate()`.\n",
      "        :param parameters: (Optional) Parameters for the LLM used to compute the metric. By default, we\n",
      "            set the temperature to 0.0, max_tokens to 200, and top_p to 1.0. We recommend\n",
      "            setting the temperature to 0.0 for the LLM used as a judge to ensure consistent results.\n",
      "        :param aggregations: (Optional) The list of options to aggregate the scores. Currently supported\n",
      "            options are: min, max, mean, median, variance, p90.\n",
      "        :param greater_is_better: (Optional) Whether the metric is better when it is greater.\n",
      "        :param max_workers: (Optional) The maximum number of workers to use for judge scoring.\n",
      "            Defaults to 10 workers.\n",
      "        \n",
      "        :return: A metric object.\n",
      "        \n",
      "        .. testcode:: python\n",
      "            :caption: Example for creating a genai metric\n",
      "        \n",
      "            from mlflow.metrics.genai import EvaluationExample, make_genai_metric\n",
      "        \n",
      "            example = EvaluationExample(\n",
      "                input=\"What is MLflow?\",\n",
      "                output=(\n",
      "                    \"MLflow is an open-source platform for managing machine \"\n",
      "                    \"learning workflows, including experiment tracking, model packaging, \"\n",
      "                    \"versioning, and deployment, simplifying the ML lifecycle.\"\n",
      "                ),\n",
      "                score=4,\n",
      "                justification=(\n",
      "                    \"The definition effectively explains what MLflow is \"\n",
      "                    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
      "                ),\n",
      "                grading_context={\n",
      "                    \"targets\": (\n",
      "                        \"MLflow is an open-source platform for managing \"\n",
      "                        \"the end-to-end machine learning (ML) lifecycle. It was developed by \"\n",
      "                        \"Databricks, a company that specializes in big data and machine learning \"\n",
      "                        \"solutions. MLflow is designed to address the challenges that data \"\n",
      "                        \"scientists and machine learning engineers face when developing, training, \"\n",
      "                        \"and deploying machine learning models.\"\n",
      "                    )\n",
      "                },\n",
      "            )\n",
      "        \n",
      "            metric = make_genai_metric(\n",
      "                name=\"answer_correctness\",\n",
      "                definition=(\n",
      "                    \"Answer correctness is evaluated on the accuracy of the provided output based on \"\n",
      "                    \"the provided targets, which is the ground truth. Scores can be assigned based on \"\n",
      "                    \"the degree of semantic similarity and factual correctness of the provided output \"\n",
      "                    \"to the provided targets, where a higher score indicates higher degree of accuracy.\"\n",
      "                ),\n",
      "                grading_prompt=(\n",
      "                    \"Answer correctness: Below are the details for different scores:\"\n",
      "                    \"- Score 1: The output is completely incorrect. It is completely different from \"\n",
      "                    \"or contradicts the provided targets.\"\n",
      "                    \"- Score 2: The output demonstrates some degree of semantic similarity and \"\n",
      "                    \"includes partially correct information. However, the output still has significant \"\n",
      "                    \"discrepancies with the provided targets or inaccuracies.\"\n",
      "                    \"- Score 3: The output addresses a couple of aspects of the input accurately, \"\n",
      "                    \"aligning with the provided targets. However, there are still omissions or minor \"\n",
      "                    \"inaccuracies.\"\n",
      "                    \"- Score 4: The output is mostly correct. It provides mostly accurate information, \"\n",
      "                    \"but there may be one or more minor omissions or inaccuracies.\"\n",
      "                    \"- Score 5: The output is correct. It demonstrates a high degree of accuracy and \"\n",
      "                    \"semantic similarity to the targets.\"\n",
      "                ),\n",
      "                examples=[example],\n",
      "                version=\"v1\",\n",
      "                model=\"openai:/gpt-4\",\n",
      "                grading_context_columns=[\"targets\"],\n",
      "                parameters={\"temperature\": 0.0},\n",
      "                aggregations=[\"mean\", \"variance\", \"p90\"],\n",
      "                greater_is_better=True,\n",
      "            )\n",
      "    \n",
      "    relevance(model: Optional[str] = None, metric_version: Optional[str] = None, examples: Optional[List[mlflow.metrics.genai.base.EvaluationExample]] = None) -> mlflow.models.evaluation.base.EvaluationMetric\n",
      "        This function will create a genai metric used to evaluate the evaluate the relevance of an\n",
      "        LLM using the model provided. Relevance will be assessed by the appropriateness, significance,\n",
      "        and applicability of the output with respect to the input and ``context``.\n",
      "        \n",
      "        The ``context`` eval_arg must be provided as part of the input dataset or output\n",
      "        predictions. This can be mapped to a column of a different name using ``col_mapping``\n",
      "        in the ``evaluator_config`` parameter.\n",
      "        \n",
      "        An MlflowException will be raised if the specified version for this metric does not exist.\n",
      "        \n",
      "        :param model: (Optional) Model uri of an openai or gateway judge model in the format of\n",
      "            \"openai:/gpt-4\" or \"gateway:/my-route\". Defaults to\n",
      "            \"openai:/gpt-4\". Your use of a third party LLM service (e.g., OpenAI) for\n",
      "            evaluation may be subject to and governed by the LLM service's terms of use.\n",
      "        :param metric_version: (Optional) The version of the relevance metric to use.\n",
      "            Defaults to the latest version.\n",
      "        :param examples: (Optional) Provide a list of examples to help the judge model evaluate the\n",
      "            relevance. It is highly recommended to add examples to be used as a reference to evaluate\n",
      "            the new results.\n",
      "        :return: A metric object\n",
      "\n",
      "DATA\n",
      "    __all__ = ['EvaluationExample', 'make_genai_metric', 'answer_similarit...\n",
      "\n",
      "FILE\n",
      "    /home/ubuntu/anaconda3/envs/py39/lib/python3.9/site-packages/mlflow/metrics/genai/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mlflow.metrics.genai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f30b0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's faithfulness score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called faithfulness based on the input and output.\n",
      "A definition of faithfulness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Faithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n",
      "\n",
      "Grading rubric:\n",
      "Faithfulness: Below are the details for different scores:\n",
      "- Score 1: None of the claims in the output can be inferred from the provided context.\n",
      "- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n",
      "- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n",
      "- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n",
      "- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "How do I disable MLflow autologging?\n",
      "\n",
      "Example Output:\n",
      "mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\n",
      "        \n",
      "\n",
      "Example Input:\n",
      "How do I disable MLflow autologging?\n",
      "\n",
      "Example Output:\n",
      "mlflow.autolog(disable=True) will disable autologging for all functions.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\n",
      "\n",
      "Example score: 5\n",
      "Example justification: The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's faithfulness score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "# Create a faithfulness metric\n",
    "\n",
    "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
    "\n",
    "# Create a good and bad example for faithfulness in the context of this problem\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n",
    "        score=2,\n",
    "        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(model=\"openai:/gpt-4\", examples=faithfulness_examples)\n",
    "print(faithfulness_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f657994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=relevance, greater_is_better=True, long_name=relevance, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's relevance based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's relevance score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called relevance based on the input and output.\n",
      "A definition of relevance and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Relevance encompasses the appropriateness, significance, and applicability of the output with respect to both the input and context. Scores should reflect the extent to which the output directly addresses the question provided in the input, given the provided context.\n",
      "\n",
      "Grading rubric:\n",
      "Relevance: Below are the details for different scores:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the provided context.\n",
      "- Score 2: The output provides some relevance to the question and is somehow related to the provided context.\n",
      "- Score 3: The output mostly answers the question and is largely consistent with the provided context.\n",
      "- Score 4: The output answers the question and is consistent with the provided context.\n",
      "- Score 5: The output answers the question comprehensively using the provided context.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Example Output:\n",
      "Databricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The output provides relevant information about Databricks, mentioning it as a company specializing in big data and machine learning solutions. However, it doesn't directly address how MLflow is related to Databricks, which is the specific question asked in the input. Therefore, the output is only somewhat related to the provided context.\n",
      "        \n",
      "\n",
      "Example Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Example Output:\n",
      "MLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 4\n",
      "Example justification: The output provides a relevant and accurate statement about the relationship between MLflow and Databricks. While it doesn't provide extensive detail, it still offers a substantial and meaningful response. To achieve a score of 5, the response could be further improved by providing additional context or details about how MLflow specifically functions within the Databricks ecosystem.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's relevance based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's relevance score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import relevance, EvaluationExample\n",
    "\n",
    "relevance_metric = relevance(model=\"openai:/gpt-4\")\n",
    "print(relevance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85c37504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py39/lib/python3.9/site-packages/mlflow/data/digest_utils.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n",
      "/home/ubuntu/anaconda3/envs/py39/lib/python3.9/site-packages/mlflow/models/evaluation/base.py:414: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(_hash_array_like_element_as_bytes)\n",
      "2023/11/19 17:42:45 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/11/19 17:42:45 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/11/19 17:42:51 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2023/11/19 17:42:51 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2023/11/19 17:42:51 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2023/11/19 17:42:51 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99baae1d59a946469371496e6f4fc61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a34f640d999445bab356bab12bdb7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/19 17:43:00 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/11/19 17:43:00 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/11/19 17:43:00 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2023/11/19 17:43:00 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/11/19 17:43:00 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2023/11/19 17:43:00 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/11/19 17:43:00 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n",
      "2023/11/19 17:43:00 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/11/19 17:43:00 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7407e0e3fd41d594c8324f6e7d3e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/19 17:43:23 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: relevance\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b789da4aec5d471493ea1756e9899fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness/v1/mean': 4.0, 'faithfulness/v1/variance': 3.0, 'faithfulness/v1/p90': 5.0, 'relevance/v1/mean': 4.5, 'relevance/v1/variance': 0.25, 'relevance/v1/p90': 5.0}\n"
     ]
    }
   ],
   "source": [
    "results = mlflow.evaluate(\n",
    "    model,\n",
    "    eval_df,\n",
    "    model_type=\"question-answering\",\n",
    "    evaluators=\"default\",\n",
    "    predictions=\"result\",\n",
    "    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"questions\",\n",
    "            \"context\": \"source_documents\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf7eac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6827c7a759b5497a878c57174389c095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>outputs</th>\n",
       "      <th>query</th>\n",
       "      <th>source_documents</th>\n",
       "      <th>latency</th>\n",
       "      <th>token_count</th>\n",
       "      <th>faithfulness/v1/score</th>\n",
       "      <th>faithfulness/v1/justification</th>\n",
       "      <th>relevance/v1/score</th>\n",
       "      <th>relevance/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open-source platform, purpose-bu...</td>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>[{'lc_attributes': {}, 'lc_secrets': {}, 'meta...</td>\n",
       "      <td>1.742607</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provided by the model is completely...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provides a comprehensive answer to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to run mlflow.evaluate()?</td>\n",
       "      <td>mlflow.evaluate() is an API that allows you t...</td>\n",
       "      <td>How to run mlflow.evaluate()?</td>\n",
       "      <td>[{'lc_attributes': {}, 'lc_secrets': {}, 'meta...</td>\n",
       "      <td>1.733826</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provided by the model is completely...</td>\n",
       "      <td>4</td>\n",
       "      <td>The output provides a relevant and accurate ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to log_table()?</td>\n",
       "      <td>You can log a table with MLflow using the log...</td>\n",
       "      <td>How to log_table()?</td>\n",
       "      <td>[{'lc_attributes': {}, 'lc_secrets': {}, 'meta...</td>\n",
       "      <td>1.221808</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>The output claims that you can log a table wit...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provides a comprehensive answer to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to load_table()?</td>\n",
       "      <td>You can't load_table() with MLflow. MLflow is...</td>\n",
       "      <td>How to load_table()?</td>\n",
       "      <td>[{'lc_attributes': {}, 'lc_secrets': {}, 'meta...</td>\n",
       "      <td>1.029105</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>The output claims that MLflow is a tool for ma...</td>\n",
       "      <td>4</td>\n",
       "      <td>The output provides a relevant and accurate re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       questions  \\\n",
       "0                What is MLflow?   \n",
       "1  How to run mlflow.evaluate()?   \n",
       "2            How to log_table()?   \n",
       "3           How to load_table()?   \n",
       "\n",
       "                                             outputs  \\\n",
       "0   MLflow is an open-source platform, purpose-bu...   \n",
       "1   mlflow.evaluate() is an API that allows you t...   \n",
       "2   You can log a table with MLflow using the log...   \n",
       "3   You can't load_table() with MLflow. MLflow is...   \n",
       "\n",
       "                           query  \\\n",
       "0                What is MLflow?   \n",
       "1  How to run mlflow.evaluate()?   \n",
       "2            How to log_table()?   \n",
       "3           How to load_table()?   \n",
       "\n",
       "                                    source_documents   latency  token_count  \\\n",
       "0  [{'lc_attributes': {}, 'lc_secrets': {}, 'meta...  1.742607           53   \n",
       "1  [{'lc_attributes': {}, 'lc_secrets': {}, 'meta...  1.733826           61   \n",
       "2  [{'lc_attributes': {}, 'lc_secrets': {}, 'meta...  1.221808           32   \n",
       "3  [{'lc_attributes': {}, 'lc_secrets': {}, 'meta...  1.029105           27   \n",
       "\n",
       "   faithfulness/v1/score                      faithfulness/v1/justification  \\\n",
       "0                      5  The output provided by the model is completely...   \n",
       "1                      5  The output provided by the model is completely...   \n",
       "2                      1  The output claims that you can log a table wit...   \n",
       "3                      5  The output claims that MLflow is a tool for ma...   \n",
       "\n",
       "   relevance/v1/score                         relevance/v1/justification  \n",
       "0                   5  The output provides a comprehensive answer to ...  \n",
       "1                   4  The output provides a relevant and accurate ex...  \n",
       "2                   5  The output provides a comprehensive answer to ...  \n",
       "3                   4  The output provides a relevant and accurate re...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0f9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
